{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Install the package\n",
    "!pip install dataprep\n",
    "!pip install scikit-survival\n",
    "!pip install lifelines\n",
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load packages\n",
    "\n",
    "# Packages to load and preprocess data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Packages to visualise and explore data\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "# from dataprep.eda import plot, create_report\n",
    "from dataprep.eda import plot, create_report, plot_missing, plot_correlation\n",
    "\n",
    "# Packages to prepare data for ML  \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Packages for survival analysis\n",
    "from lifelines import CoxPHFitter \n",
    "from lifelines.utils import k_fold_cross_validation\n",
    "from lifelines.statistics import logrank_test\n",
    "from lifelines import KaplanMeierFitter \n",
    "from lifelines.plotting import add_at_risk_counts\n",
    "\n",
    "# Packages for ML in survival analysis\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.svm import FastSurvivalSVM\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "# Package to interpret data\n",
    "import shap\n",
    "\n",
    "\n",
    "# Ignore the warnings notifications\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Connect to google drive \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load data\n",
    "file1 = pd.read_csv('/content/drive/MyDrive/Data_Tutorial_BC/data_clinical_patient.csv')\n",
    "file2 = pd.read_csv('/content/drive/MyDrive/Data_Tutorial_BC/data_clinical_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Merge clinical data\n",
    "data = pd.merge(file1,file2, how=\"inner\", on=[\"PATIENT_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a quick look on data\n",
    "data.info()\n",
    "\n",
    "# Data information\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Preprocess data & Explore data\n",
    "\n",
    "# 4.1 Check duplicated values\n",
    "print('The number of duplicated values in data:',\n",
    "         data.duplicated().sum())\n",
    "\n",
    "# 4.2 Drop unused cols: Based on data.info(), we will drop some unused cols and null cols\n",
    "\n",
    "drop_list = ['VITAL_STATUS', 'SAMPLE_ID', 'SEX', 'SAMPLE_TYPE',\n",
    "            'RFS_STATUS', 'RFS_MONTHS']\n",
    "data = data.drop(drop_list, axis=1)\n",
    "\n",
    "# We check the number of patients by cancer type\n",
    "print('\\nGroup Patients by',data.groupby('CANCER_TYPE')['PATIENT_ID'].count())\n",
    "\n",
    "# There are only three patients with Breast Sarcoma\n",
    "# So we will filter those patients with Breast Cancer type\n",
    "data = data[data['CANCER_TYPE'] == 'Breast Cancer']\n",
    "\n",
    "# Delete Cancer type columns as this column reports the same\n",
    "# value for all the samples, and it does not bring any useful\n",
    "# information for the following steps of the analysis\n",
    "data = data.drop(['CANCER_TYPE'], axis=1)\n",
    "print('\\nAfter the preprocessing, the shape of data is:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3: Understand data\n",
    "# Save to report as html file\n",
    "create_report(data).save('/content/drive/MyDrive/Data_Tutorial_BC/EDA_clinical_report')\n",
    "\n",
    "# Optional to explore parts of the report\n",
    "plot_missing(data).save('/content/drive/MyDrive/Data_Tutorial_BC/missing_values.html')\n",
    "plot(data).save('/content/drive/MyDrive/Data_Tutorial_BC/data.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.4: Deal with missing values\n",
    "\n",
    "# There is no columns more than 50% missing value\n",
    "cols_mv_50 = data.columns[data.isnull().mean()>0.5]\n",
    "print('Number of columns having more 50% missing data:', len(cols_mv_50))\n",
    "\n",
    "# Remove row with more than 50% missing\n",
    "percent = 50\n",
    "min_count =  int(((100-percent)/100)*data.shape[1] + 1)\n",
    "data = data.dropna(axis=0, thresh=min_count)\n",
    "print('After removing rows with more than 50% missing values:', data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print columns name having blanks\n",
    "cols_missvalue = data.columns[data.isnull().sum()>0]\n",
    "print('List columns having missing data:', cols_missvalue)\n",
    "\n",
    "cat_var = ['LYMPH_NODES_EXAMINED_POSITIVE', 'CELLULARITY', 'ER_IHC',\n",
    "            'THREEGENE', 'LATERALITY', 'HISTOLOGICAL_SUBTYPE',\n",
    "            'BREAST_SURGERY', 'GRADE', 'TUMOR_STAGE']\n",
    "num_var = ['TUMOR_SIZE']\n",
    "\n",
    "# Replace missing values with most frequent values\n",
    "data[cat_var] = data[cat_var].fillna(data[cat_var].mode().iloc[0])\n",
    "\n",
    "# Replace missing values with average values\n",
    "data[num_var] = data[num_var].fillna(data[num_var].mean())\n",
    "\n",
    "# Check missing values again\n",
    "print('After preprocessing, missing value number:', data.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.5: Exploring data\n",
    "plot(data.iloc[:,1:]).save('Plot\\preprocessed_data.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 Encode Categorical data\n",
    "\n",
    "# Encode OS status to dummy\n",
    "data['OS_STATUS'] = np.where(data['OS_STATUS'] == '1:DECEASED', 1, 0)\n",
    "\n",
    "# Encode other categorical variables\n",
    "other_var = ['LYMPH_NODES_EXAMINED_POSITIVE', 'NPI','AGE_AT_DIAGNOSIS', 'COHORT', 'GRADE',\n",
    "            'TUMOR_SIZE', 'TUMOR_STAGE', 'TMB_NONSYNONYMOUS','OS_MONTHS', 'OS_STATUS','PATIENT_ID']\n",
    "df_encode = data.drop(other_var, axis=1)\n",
    "\n",
    "# Some variables are not in order, so we have to specify the variables and their corresponding order \n",
    "modified_list =['CELLULARITY', 'HER2_SNP6', 'INFERRED_MENOPAUSAL_STATE', 'INTCLUST', 'THREEGENE']\n",
    "keep_list = df_encode.columns[~df_encode.columns.isin(modified_list)]\n",
    "cel_cat = ['Low', 'Moderate', 'High']\n",
    "her2_cat = ['UNDEF','LOSS', 'NEUTRAL', 'GAIN']\n",
    "inf_cat = ['Pre', 'Post']\n",
    "intclust_cat = ['1', '2', '3', '4ER+', '4ER-', '5', '6', '7', '8', '9', '10']\n",
    "three_gene_cat = ['ER-/HER2-', 'HER2+', 'ER+/HER2- Low Prolif', 'ER+/HER2- High Prolif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the predefined order variables \n",
    "enc = OrdinalEncoder(categories=[cel_cat, her2_cat, inf_cat, intclust_cat, three_gene_cat]).fit(df_encode[modified_list])\n",
    "encoder = enc.transform(df_encode[modified_list])\n",
    "df_encode_new = pd.DataFrame(encoder, columns=modified_list)\n",
    "\n",
    "# Encode the other variables \n",
    "enc1 = OrdinalEncoder().fit(df_encode[keep_list])\n",
    "encoder1 = enc1.transform(df_encode[keep_list])\n",
    "df_encode_new1 = pd.DataFrame(encoder1, columns=keep_list)\n",
    "\n",
    "# Merge encode data and original data\n",
    "df =pd.concat([df_encode_new, df_encode_new1, data[other_var].reset_index(drop=True)], axis=1)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the encoded categories\n",
    "for i in range(len(modified_list)):\n",
    "    print(modified_list[i], enc.categories_[i])\n",
    "for i in range(len(keep_list)):\n",
    "    print(keep_list[i], enc1.categories_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocess data to csv to merge to gene data\n",
    "df.to_csv('/content/drive/MyDrive/Data_Tutorial_BC/clinical.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Patient ID column as this is not relevant for the analysis\n",
    "df = df.drop(['PATIENT_ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.7 Correlation analysis\n",
    "colormap = plt.cm.Reds\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(df.corr(),linewidths=0.1,vmax=0.8, \n",
    "            square=True, cmap = colormap, linecolor='white')\n",
    "plt.title('Correlation matrix', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.7 Time Distribution of Death and Censor\n",
    "num_censored = df.shape[0] - df[\"OS_STATUS\"].sum()\n",
    "print(\"%.1f%% of records are censored\" % (num_censored/df.shape[0]*100))\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "val, bins, patches = plt.hist((df.query('OS_STATUS == 1')['OS_MONTHS'],\n",
    "                               df.query('OS_STATUS == 0')['OS_MONTHS']),\n",
    "                              bins=30, stacked=True)\n",
    "_ = plt.legend(patches, [\"Time of Deaths\", \"Time of Censoring\"])\n",
    "plt.title(\"Time Distribution for Censored and Death Patients\")\n",
    "plt.xlabel('Time (months)',fontsize='large')\n",
    "plt.ylabel('Frequency',fontsize='large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After preprocessing, the shape of dataset\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Cox survival analysis\n",
    "# 5.1: Normalise data\n",
    "ss = MinMaxScaler()\n",
    "df_norm = df.drop(['OS_STATUS', 'OS_MONTHS'], axis = 1)\n",
    "df_norm = pd.DataFrame(ss.fit_transform(df_norm), columns=df_norm.columns)\n",
    "df_norm['OS_STATUS'] = df['OS_STATUS']\n",
    "df_norm['OS_MONTHS'] = df['OS_MONTHS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2: Build model \n",
    "# Cox Proportional Hazards Model\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(df_norm, duration_col='OS_MONTHS', event_col='OS_STATUS')\n",
    "\n",
    "# Plot\n",
    "\n",
    "plt.figure(figsize=(7, 9))\n",
    "plt.title('Cox Proportional Hazards Model for Clinical data')\n",
    "cph.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report\n",
    "cph.print_summary(columns=[\"coef\",\"exp(coef)\",\"exp(coef) lower 95%\",\"exp(coef) upper 95%\", \"z\", \"p\"], decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation (optional)\n",
    "scores = k_fold_cross_validation(cph, df_norm, 'OS_MONTHS', event_col='OS_STATUS', k=5, \n",
    "                                scoring_method=\"concordance_index\", seed=18)\n",
    "\n",
    "print(\"Average score\", round(np.mean(scores),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Machine Learning Methods for Survival Analysis\n",
    "\n",
    "# 6.1: Set up seed and the options for the cross-validation approach\n",
    "SEED = 5\n",
    "CV = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# 6.2 Split data to prepare for ML\n",
    "X = df.drop(['OS_MONTHS','OS_STATUS'], axis = 1)\n",
    "df['OS_STATUS'] = np.where(df['OS_STATUS'] == 1, True, False)\n",
    "y = df[['OS_STATUS','OS_MONTHS']].to_records(index=False)\n",
    "\n",
    "# Split the data set into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    stratify=y['OS_STATUS'],\n",
    "                                                    random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3: Build model\n",
    "# Define a function for grid search to tune training model\n",
    "# and predict the results\n",
    "def grid_search(estimator, param, X_train, y_train, X_test, y_test, CV):\n",
    "    \n",
    "    # Define Grid Search\n",
    "    gcv = GridSearchCV(\n",
    "    estimator,\n",
    "    param_grid=param,\n",
    "    cv=CV,\n",
    "    n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "    # Find best model\n",
    "    model = gcv.best_estimator_\n",
    "    print(model)\n",
    "    \n",
    "    # Predict model\n",
    "    prediction = model.predict(X_test)\n",
    "    result = concordance_index_censored(y_test[\"OS_STATUS\"], y_test[\"OS_MONTHS\"],\n",
    "                                        prediction)\n",
    "    print('C-index for test set (Hold out):', result[0])\n",
    "\n",
    "    return [model,  prediction]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run experiment 20 times \n",
    "def c_index(model, X, y, n=20):\n",
    "    np.random.seed(1)\n",
    "    seeds = np.random.permutation(1000)[:n]\n",
    "\n",
    "    # Train and evaluate model with 20 times\n",
    "    cindex_score = []\n",
    "    predict_list = []\n",
    "    \n",
    "    for s in seeds:\n",
    "        X_trn, X_test, y_trn, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                        stratify=y['OS_STATUS'],\n",
    "                                                        random_state=s)\n",
    "        model.fit(X_trn, y_trn)\n",
    "        prediction = model.predict(X_test)\n",
    "        predict_list.append(prediction)\n",
    "        result = concordance_index_censored(y_test[\"OS_STATUS\"],y_test[\"OS_MONTHS\"], prediction)\n",
    "    \n",
    "        cindex_score.append(round(result[0],3))\n",
    "\n",
    "    print('Average C-index for {} runs'.format(n), np.mean(cindex_score))\n",
    "    \n",
    "    return [cindex_score, predict_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Pipeline and hyperparameter\n",
    "\n",
    "# CoxPHSurvivalAnalysis\n",
    "pipe_cox = Pipeline([('scaler', MinMaxScaler()),('model', CoxPHSurvivalAnalysis())])\n",
    "param_cox ={'scaler': [MinMaxScaler()],\n",
    "        \"model__alpha\": [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Random Survival Forest\n",
    "pipe_rsf = Pipeline([('scaler', MinMaxScaler()),('model', RandomSurvivalForest())])\n",
    "param_rsf ={'scaler': [MinMaxScaler()],\n",
    "        'model__random_state': [SEED],\n",
    "        'model__max_features': ['sqrt'],\n",
    "        'model__max_depth': [8],\n",
    "        'model__min_samples_leaf': [50, 100],\n",
    "        'model__min_samples_split': [100],\n",
    "        'model__n_estimators':[500]}\n",
    "\n",
    "# Gradient Boost Survival\n",
    "pipe_gbs = Pipeline([('scaler', MinMaxScaler()),('model', GradientBoostingSurvivalAnalysis())])\n",
    "param_gbs ={'scaler': [MinMaxScaler()],\n",
    "        'model__random_state': [SEED],\n",
    "        'model__learning_rate': [0.01, 0.1, 1],\n",
    "        'model__n_estimators':[200, 500, 800, 1000]}\n",
    "\n",
    "# Survival SVM\n",
    "pipe_svm = Pipeline([('scaler', MinMaxScaler()),('model', FastSurvivalSVM())])\n",
    "param_svm ={'scaler': [MinMaxScaler()],\n",
    "        'model__random_state': [SEED],\n",
    "        'model__max_iter': [500, 5000],\n",
    "        'model__optimizer':['avltree', 'rbtree','simple']}\n",
    "\n",
    "# Estimator list:\n",
    "estimator_list = {'Cox Regression':[pipe_cox, param_cox ], \n",
    "                'Random Forest Survival':[pipe_rsf, param_rsf], \n",
    "                'Gradient Boosting Survival': [pipe_gbs, param_gbs], \n",
    "                'SVM Survival': [pipe_svm, param_svm]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []\n",
    "pred_list = []\n",
    "c_index_list = []\n",
    "pred_list_n = []\n",
    "\n",
    "for model_name, index in estimator_list.items():\n",
    "    print('\\n',model_name)\n",
    "    estimator = index[0]\n",
    "    param = index[1]\n",
    "    outcome = grid_search(estimator, param, X_train, y_train, X_test, y_test, CV)\n",
    "    model = outcome[0]\n",
    "    model_list.append(model)\n",
    "    pred_list.append(outcome[1])\n",
    "\n",
    "    # Run model n times to check c-index\n",
    "    score, pre = c_index(model, X, y, n=20)\n",
    "    c_index_list.append(score)\n",
    "    pred_list_n.append(pre)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise results\n",
    "name = ['CPH', 'RSF', 'GBS', 'SSVM']\n",
    "cv_res = []\n",
    "\n",
    "for i in range(0,4):\n",
    "    for c in c_index_list[i]:\n",
    "        cv_res.append([name[i],c])\n",
    "\n",
    "c_plot = pd.DataFrame(cv_res, columns=['Model Name','C-index'])\n",
    "\n",
    "plt.subplots(figsize=(8,6))\n",
    "ax = sns.boxplot(x=\"Model Name\", y=\"C-index\", data=c_plot)\n",
    "plt.title('C-index for 20 runs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KM Curve for median\n",
    "fig, ax = plt.subplots(2,2,figsize=(12,12))\n",
    "k  = 0\n",
    "for pred in pred_list:\n",
    "    df1 = X_test.reset_index(drop=True) \n",
    "    risk =[]\n",
    "    \n",
    "    y_pred = pred\n",
    "    med = np.median(y_pred)\n",
    "    r = np.where(y_pred >= med, 1, 0)\n",
    "\n",
    "    df1['Risk'] = r\n",
    "    print(df1.shape)\n",
    "    ix = df1['Risk'] == 1\n",
    "\n",
    "    df_y = pd.DataFrame(y_test)\n",
    "    df_y['OS_STATUS'] = np.where(df_y['OS_STATUS'] == True, 1, 0)\n",
    "    df1['OS_STATUS']= df_y['OS_STATUS']\n",
    "    df1['OS_MONTHS']= df_y['OS_MONTHS']\n",
    "    T_hr, E_hr = df1.loc[ix]['OS_MONTHS'], df1.loc[ix]['OS_STATUS']\n",
    "    T_lr, E_lr = df1.loc[~ix]['OS_MONTHS'], df1.loc[~ix]['OS_STATUS']\n",
    "\n",
    "    # Set-up plots\n",
    "    k+=1\n",
    "    plt.subplot(2,2,k)\n",
    "\n",
    "    # Fit survival curves\n",
    "    kmf_hr = KaplanMeierFitter()\n",
    "    ax = kmf_hr.fit(T_hr, E_hr, label='HR').plot_survival_function()\n",
    "\n",
    "    kmf_lr = KaplanMeierFitter()\n",
    "    ax = kmf_lr.fit(T_lr, E_lr, label='LR').plot_survival_function()\n",
    "\n",
    "    add_at_risk_counts(kmf_lr, kmf_lr)\n",
    "    # Format graph\n",
    "    plt.ylim(0,1)\n",
    "    ax.set_xlabel('Time (months)',fontsize='large')\n",
    "    ax.set_ylabel('Est. probability of survival',fontsize='large')\n",
    "\n",
    "    # Calculate p-value\n",
    "    res = logrank_test(T_hr, T_lr, event_observed_A=E_hr, event_observed_B=E_lr, alpha=.95)\n",
    "    print('\\nModel', name[k-1])\n",
    "    res.print_summary()\n",
    "\n",
    "    # Location the label at the 1st out of 9 tick marks\n",
    "    xloc = max(np.max(T_hr),np.max(T_lr)) / 10\n",
    "    ax.text(xloc,.2,'p-value = {:0.3e}'.format(res.p_value),fontsize=12)\n",
    "    ax.set_title('KM Curves {}' .format(name[k-1]))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Plot\\Exp1_KM.pdf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Interpret data\n",
    "# Initialize JS For Plot\n",
    "shap.initjs()\n",
    "\n",
    "for i in range(0,4):\n",
    "    print('\\nModel', name[i])\n",
    "    m = model_list[i][1]\n",
    "    m.fit(X_train,y_train)\n",
    "    explainer = shap.Explainer(m.predict, X_train, feature_names=X_train.columns)\n",
    "    shaps = explainer(X_test)\n",
    "    shap.summary_plot(shaps, X_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "09ae5ccd45b6600efb8e5e26017af87c0f22937b55ae122cad02456dfe54338e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
